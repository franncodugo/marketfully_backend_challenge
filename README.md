# üè† Marketfully Backend Challenge

Soluci√≥n para la gesti√≥n de 2.2M+ registros de propiedades, integrando scraping demogr√°fico inteligente y procesamiento de flujos de datos.

## üéØ Decisiones de Dise√±o y Trade-offs

Para este desaf√≠o, se priorizo la eficiencia computacional y la robustez sobre la simplicidad superficial:

#### ETL con Node.js Streams: 
En lugar de cargar el CSV de 2.2M de filas en memoria, implement√© un pipeline de lectura y transformaci√≥n por bloque. Esto mantiene el consumo estable durante proceso de ingesta.

#### Cach√© H√≠brido Persistente: 
Siguiendo la restricci√≥n de "ser amables con ZipWho", implement√© una tabla zip_cache en SQLite. 

#### SQL Din√°mico vs ORM: 
Evit√© el uso de ORMs pesados. Las consultas se construyen din√°micamente con better-sqlite3 para maximizar la velocidad de ejecuci√≥n y tener control total sobre los √≠ndices.

#### Separation of Concerns: 
El c√≥digo est√° desacoplado siguiendo patrones de servicio para que la l√≥gica de scraping no contamine las rutas de la API.

## üõ†Ô∏è Stack

    Runtime: Node.js (TypeScript).

    Framework: Fastify.

    DB: SQLite better-sqlite3.

    Scraping: Cheerio.

## üöÄ Gu√≠a de Inicio R√°pido

### 1. Instalaci√≥n
```bash
npm install
```
### 2. Ingesta de Datos (ETL)

Este proceso descarga el CSV de S3, mapea estados, calcula m√©tricas (precio por acre/sqft) y crea la DB.
Bash
```bash
npm run ingest
```
### 3. Ejecuci√≥n del Servidor
Bash
```bash
npm run dev
```
### 4. Documentaci√≥n y Pruebas

Accede a la documentaci√≥n interactiva de Swagger:
üëâ http://localhost:3000/docs
Pruebas de Calidad (Smoke Tests)

Para validar la integridad de la base de datos, la conectividad del Scraper y el enriquecimiento de datos:
Bash
```bash
npm run smoke-test
```
### ‚ö†Ô∏è Notas sobre el Scraping (ZipWho)
El servicio `ScraperService` fue dise√±ado con una arquitectura de doble validaci√≥n (DOM + Regex) para maximizar la resiliencia. Sin embargo, al parecer, ZipWho.com presenta ocasionalmente cierta inestabilidad o bloqueos por IP.
- Se implement√≥ **SQLite Caching** para mitigar esto y garantizar que la API responda de una vez obtenidos los datos.
- En caso de un 404 persistente en un ZIP v√°lido, es probable que la IP del servidor haya sido agregada o registrada por rate-limit desde el proveedor externo.