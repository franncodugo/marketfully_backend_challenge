# ğŸ  Marketfully Backend Challenge

SoluciÃ³n para la gestiÃ³n de 2.2M+ registros de propiedades, integrando scraping demogrÃ¡fico inteligente y procesamiento de flujos de datos.

## ğŸ¯ Decisiones de DiseÃ±o y Trade-offs

Para este desafÃ­o, se priorizo la eficiencia computacional y la robustez sobre la simplicidad superficial:

#### ETL con Node.js Streams: 
En lugar de cargar el CSV de 2.2M de filas en memoria, implementÃ© un pipeline de lectura y transformaciÃ³n por bloque. Esto mantiene el consumo estable durante proceso de ingesta.

#### CachÃ© HÃ­brido Persistente: 
Siguiendo la restricciÃ³n de "ser amables con ZipWho", implementÃ© una tabla zip_cache en SQLite. 

#### SQL DinÃ¡mico vs ORM: 
EvitÃ© el uso de ORMs pesados. Las consultas se construyen dinÃ¡micamente con better-sqlite3 para maximizar la velocidad de ejecuciÃ³n y tener control total sobre los Ã­ndices.

#### Separation of Concerns: 
El cÃ³digo estÃ¡ desacoplado siguiendo patrones de servicio para que la lÃ³gica de scraping no contamine las rutas de la API.

## ğŸ› ï¸ Stack

    Runtime: Node.js (TypeScript).

    Framework: Fastify.

    DB: SQLite better-sqlite3.

    Scraping: Cheerio.

## ğŸš€ GuÃ­a de Inicio RÃ¡pido

### 1. InstalaciÃ³n
```bash
npm install
```
### 2. Ingesta de Datos (ETL)

Este proceso descarga el CSV de S3, mapea estados, calcula mÃ©tricas (precio por acre/sqft) y crea la DB.
Bash
```bash
npm run ingest
```
### 3. EjecuciÃ³n del Servidor
Bash
```bash
npm run dev
```
### 4. DocumentaciÃ³n y Pruebas

Accede a la documentaciÃ³n interactiva de Swagger:
ğŸ‘‰ http://localhost:3000/docs
Pruebas de Calidad (Smoke Tests)

Para validar la integridad de la base de datos, la conectividad del Scraper y el enriquecimiento de datos:
Bash
```bash
npm run smoke-test
```